import os

BOT_NAME = "bookScrape"

SPIDER_MODULES = ["bookScrape.spiders"]
NEWSPIDER_MODULE = "bookScrape.spiders"

ADDONS = {}

ROBOTSTXT_OBEY = False

CONCURRENT_REQUESTS = 16
CONCURRENT_REQUESTS_PER_DOMAIN = 2

DOWNLOAD_DELAY = 0
AUTOTHROTTLE_ENABLED = True

AUTOTHROTTLE_START_DELAY = 1.5
AUTOTHROTTLE_MAX_DELAY = 20
AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
AUTOTHROTTLE_DEBUG = False


RETRY_ENABLED = True
RETRY_TIMES = 2
DOWNLOAD_TIMEOUT = 15

COOKIES_ENABLED = False
TELNETCONSOLE_ENABLED = False

SPIDER_MIDDLEWARES = {}
DOWNLOADER_MIDDLEWARES = {
    "bookScrape.middlewares.ScrapeOpsHeadersMiddleware": 300,
    "bookScrape.middlewares.ScrapeOpsProxyFallbackMiddleware": 400
}

EXTENSIONS = {}

ITEM_PIPELINES = {}



HTTPCACHE_ENABLED = False

FEED_EXPORT_ENCODING = "utf-8"
DUPEFILTER_DEBUG = True

SCRAPEOPS_NUM_RESULTS = 100
SCRAPEOPS_API_KEY = os.getenv("SCRAPEOPS_API_KEY")

DB_HOST = os.getenv("DB_HOST")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_NAME = os.getenv("DB_NAME")

LOG_LEVEL = 'INFO'
#LOG_FILE = "scrapy_errors.log"